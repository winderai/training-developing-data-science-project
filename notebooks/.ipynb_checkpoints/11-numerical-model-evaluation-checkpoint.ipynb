{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Model Evaluation\n",
    "\n",
    "We saw in the slides how we need some way of numerically comparing models. This section provides some hands on experience with accuracy, expected value, cost/benefits, imbalance and lots of other evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "First, let's take an in depth look at accuracy. Given the confusion matrix below, calculate the accuracy.\n",
    "\n",
    "I'm representing positive as 'a' - affirmative, and 'n' as negative. This is because of the natural ordering would make the confusion matrix confusing. (i.e. if we use y and n, y would be at the bottom!)\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Calculate the accuracy (sklearn.metrics or DIY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  3]\n",
      " [ 5 30]]\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "y_true = ['a'] * 15 + ['n'] * 35\n",
    "y_pred = ['a'] * 12 + ['n'] * 3 + ['n'] * 30 + ['a'] * 5\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "print(metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have quite a lot of skew here too.\n",
    "\n",
    "## Expected Value\n",
    "\n",
    "Imagine we had a marketing example, like in the slides. The idea is that we want to spend some money on marketing, but we only want to target people that make sense. We were given the following information:\n",
    "\n",
    "- Profit from each sale: £50\n",
    "- Cost for marketing: £9\n",
    "\n",
    "We can generate a cost/benefit matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41 -9]\n",
      " [ 0  0]]\n"
     ]
    }
   ],
   "source": [
    "profit = 50\n",
    "cost   = -9\n",
    "cost_benefit = np.array([[profit+cost, cost],[0   , 0]])\n",
    "print(cost_benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results in the previous confusion matrix, what is the expected value?\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Calculate the expected profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3\n"
     ]
    }
   ],
   "source": [
    "def expected_value(confusion_matrix, cost_benefit_matrix):\n",
    "    return sum(sum(confusion_matrix * cost_benefit_matrix)) / sum(sum(confusion_matrix))\n",
    "\n",
    "print(expected_value(cm, cost_benefit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Arrays\n",
    "\n",
    "Lets take a look at the two confusion matricies seen in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a:\n",
      " [[25 30]\n",
      " [ 0 45]]\n",
      "model b:\n",
      " [[30  0]\n",
      " [20 50]]\n"
     ]
    }
   ],
   "source": [
    "model_a = np.array([[25, 30], [0, 45]])\n",
    "model_b = np.array([[30, 0], [20, 50]])\n",
    "print(\"model a:\\n\", model_a)\n",
    "print(\"model b:\\n\", model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "\n",
    "- Calculate the expected value for the two models, using the cost-benefit matrix from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a:\n",
      " 7.55\n",
      "model b:\n",
      " 12.3\n"
     ]
    }
   ],
   "source": [
    "print(\"model a:\\n\", expected_value(model_a, cost_benefit))\n",
    "print(\"model b:\\n\", expected_value(model_b, cost_benefit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But look at the sizes of each model test set, there's a big skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a sample size:\n",
      " [25 75]\n",
      "model b sample size:\n",
      " [50 50]\n"
     ]
    }
   ],
   "source": [
    "print(\"model a sample size:\\n\", sum(model_a))\n",
    "print(\"model b sample size:\\n\", sum(model_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factoring Out Sample Size\n",
    "\n",
    "Remember we can factor out the sample size with the probability identity:\n",
    "\n",
    "$p(\\mathbf{Y},\\mathbf{n}) = p(\\mathbf{n})\\cdotp(\\mathbf{Y} \\vert \\mathbf{n})$\n",
    "\n",
    "Which means we can factor out the sample sizes with:\n",
    "\n",
    "\\begin{align}\\\\\\\\\n",
    "\\text{expected profit} = &\n",
    "    p(\\mathbf{p}) \\cdot \\left[\n",
    "        p(\\mathbf{Y} \\vert \\mathbf{p}) \\cdot b(\\mathbf{Y},\\mathbf{p}) +\n",
    "        p(\\mathbf{N} \\vert \\mathbf{p}) \\cdot b(\\mathbf{N},\\mathbf{p})\n",
    "    \\right] + \\\\\\\\\n",
    "&\n",
    "    p(\\mathbf{n}) \\cdot  \\left[\n",
    "        p(\\mathbf{Y} \\vert \\mathbf{n}) \\cdot b(\\mathbf{Y},\\mathbf{n}) +\n",
    "        p(\\mathbf{N} \\vert \\mathbf{n}) \\cdot b(\\mathbf{N},\\mathbf{n})\n",
    "     \\right]\\\\\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be equal: 7.55 7.55\n",
      "Should be equal: 12.3 12.3\n"
     ]
    }
   ],
   "source": [
    "def factored_expected_value(m, cb, p_p=0.5, p_n=0.5):   \n",
    "    t_p = sum(m[:,0])\n",
    "    t_n = sum(m[:,1])\n",
    "    return p_p * (m[0,0]/t_p) * cb[0,0] + (m[1,0]/t_p) * cb[1,0] + \\\n",
    "           p_n * (m[0,1]/t_n) * cb[0,1] + (m[1,1]/t_n) * cb[1,1]\n",
    "\n",
    "print(\"Should be equal:\", expected_value(model_a, cost_benefit), factored_expected_value(model_a, cost_benefit, 0.25, 0.75))\n",
    "print(\"Should be equal:\", expected_value(model_b, cost_benefit), factored_expected_value(model_b, cost_benefit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after factoring out training sample skew\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Results after factoring out training sample skew\")\n",
    "print(\"================================================\")\n",
    "# Code for non-skewed model A\n",
    "# Code for non-skewed model B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "- Add the expected value calculation above whilst factoring out the skew (set the classes to 0.5 each). What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Evaluation Metrics\n",
    "\n",
    "Let's take a quick look at some other evaluation metrics.\n",
    "\n",
    "$$ \\text{accuracy}\n",
    "= \\frac{\\text{correct predictions}}{\\text{all instances}}\n",
    "= \\frac{TP+TN}{P+N}\n",
    "= \\frac{TP+TN}{TP+FP+TN+FN} $$\n",
    "\n",
    "$$ \\text{precision}\n",
    "= \\frac{\\text{true positives}}{\\text{all predicted yes}}\n",
    "= \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$ \\text{recall} = \\frac{\\text{true positives}}{\\text{all positives}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "$$ \\text{false positive rate}\n",
    "= \\frac{\\text{false positives}}{\\text{all negatives}}\n",
    "= \\frac{FP}{N}\n",
    "= \\frac{FP}{FP+TN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a:\n",
      " [[25 30]\n",
      " [ 0 45]] \n",
      " [['TP' 'FN']\n",
      " ['FP' 'TN']] \n",
      "\n",
      "Accuracy: 0.7\n",
      "Precision: 1.0\n",
      "Recall: 0.454545454545\n",
      "FPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"model a:\\n\", model_a, \"\\n\", np.array([['TP', 'FN'],['FP', 'TN']]), \"\\n\")\n",
    "\n",
    "TP = model_a[0,0]; FP = model_a[1,0];\n",
    "TN = model_a[1,1]; FN = model_a[0,1]\n",
    "\n",
    "print(\"Accuracy:\", (TP+TN)/(TP+FP+TN+FN) )\n",
    "print(\"Precision:\", (TP)/(TP+FP) )\n",
    "print(\"Recall:\", (TP)/(TP+FN) )\n",
    "print(\"FPR:\", (FP)/(FP+TN) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = ['a'] * 25 + ['n'] * 75\n",
    "y_pred = ['a'] * 20 + ['n'] * 30 + ['n'] * 15 + ['a'] * 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- Create a confusion matrix for the above data\n",
    "- Calculate the Accuracy, prevision, recall and FPR\n",
    "\n",
    "## Bonus\n",
    "\n",
    "If you finish early, go grab a coffee, or try these tasks:\n",
    "\n",
    "- Bring up the digits dataset again\n",
    "- Try generating all these metrics (accuracy, precision, recall, etc.) for that dataset.\n",
    "- Investigate what other metrics sklearn as to offer and try them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
